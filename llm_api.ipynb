{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7079478",
   "metadata": {},
   "source": [
    "# uv for pacakge management\n",
    "\n",
    "```\n",
    "uv sync\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedcc052",
   "metadata": {},
   "source": [
    "# Ollama and Local models\n",
    "\n",
    "https://ollama.com/\n",
    "\n",
    "https://ollama.com/library\n",
    "\n",
    "```\n",
    "ollama run gemma3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37233b7e",
   "metadata": {},
   "source": [
    "# API from frontier model providers\n",
    "\n",
    "Following OpenAI's API format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab4b97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2834c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùåOpenAI API Key not set\n",
      "‚ùåAnthropic API Key not set\n",
      "‚úÖGoogle API Key exists and begins AIzaSyCo\n",
      "‚ùåGroq API Key not set\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"‚úÖOpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"‚ùåOpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"‚úÖAnthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"‚ùåAnthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"‚úÖGoogle API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"‚ùåGoogle API Key not set\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"‚úÖGroq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"‚ùåGroq API Key not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0582187a",
   "metadata": {},
   "source": [
    "# Basic LLM Call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad0ee634",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = OpenAI(base_url=gemini_url, api_key=google_api_key)\n",
    "ollama = OpenAI(base_url=\"http://localhost:11434/v1/\", api_key=\"\")  # No API key needed for Ollama\n",
    "\n",
    "\n",
    "\n",
    "#openai = OpenAI(api_key=openai_api_key)\n",
    "#anthropic = OpenAI(base_url=anthropic_url, api_key=anthropic_api_key)\n",
    "#groq = OpenAI(base_url=groq_url, api_key=groq_api_key)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe23bb6a",
   "metadata": {},
   "source": [
    "# System Prompt and User Prompt\n",
    "\n",
    "| Aspect | System Prompt | User Prompt |\n",
    "|---|---|---|\n",
    "| Primary Purpose | Define the model‚Äôs role, behavior, boundaries, and global rules | Specify the actual task, question, or request |\n",
    "| Provided By | System / Developer | End user |\n",
    "| Scope of Influence | Global (affects the entire conversation) | Local (affects only the current request) |\n",
    "| Priority | **Highest** (overrides all other prompts) | Lower (must comply with the system prompt) |\n",
    "| Typical Content | Role definition, tone, style, constraints, safety rules | Questions, instructions, data, requirements |\n",
    "| Visibility to User | Usually hidden from the user | Visible to the user |\n",
    "| Usage Frequency | Usually set once at the start of a conversation | Can change every turn |\n",
    "| Example | ‚ÄúYou are a strict Python tutor. Respond in clear, concise English.‚Äù | ‚ÄúExplain Python generators with examples.‚Äù |\n",
    "| Design Focus | Consistency, safety, and behavior control | Clarity and task specificity |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c5229f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As a hypothetical Martian, you‚Äôd likely experience a life under a reddish sky, with a much weaker gravity, colder temperatures, and a thin atmosphere, all while navigating a landscape sculpted by ancient volcanoes and vast canyons.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful assistant. explain in 1 sentence.\"\n",
    "question = 'What is it like to be a martian?'\n",
    "\n",
    "\n",
    "response = ollama.chat.completions.create(\n",
    "        model = 'gemma3',\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    ")\n",
    "\n",
    "#response # Output\n",
    "response.choices[0].message.content  # What we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72069ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='wmJTaf_jH4Pdvr0Plq2V4Ao', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='One would simply exist, alone, on a planet where life never truly blossomed, a silent testament to cosmic indifference.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1767072450, model='gemini-2.5-flash', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=23, prompt_tokens=24, total_tokens=375, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"You are a sad scientist. Answer in 1 sentence.\"\n",
    "\n",
    "response = gemini.chat.completions.create(\n",
    "        model = 'gemini-2.5-flash',\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"What is it like to be a martian?\"}\n",
    "        ]\n",
    ")\n",
    "response # Output\n",
    "#response.choices[0].message.content  # What we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7855094c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='wmJTaf_jH4Pdvr0Plq2V4Ao', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='One would simply exist, alone, on a planet where life never truly blossomed, a silent testament to cosmic indifference.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1767072450, model='gemini-2.5-flash', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=23, prompt_tokens=24, total_tokens=375, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5884c0c1",
   "metadata": {},
   "source": [
    "# Stateless of LLM and illusion of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b415b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_ollama(user_input):\n",
    "    response = ollama.chat.completions.create(\n",
    "        model = 'gemma3',\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceb99cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Adam, I‚Äôm here to assist you with information and tasks ‚Äì just let me know what you need!\n",
      "I do not know your name, as I have no way of knowing personal information about you. üòä\n"
     ]
    }
   ],
   "source": [
    "response1 = chat_with_ollama(\"Hi, I am Adam\")\n",
    "print(response1)\n",
    "\n",
    "response2 = chat_with_ollama(\"What is my name?\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29669871",
   "metadata": {},
   "source": [
    "## LLM chat that seems to have memory\n",
    "\n",
    "Actually, we just pack the whole conversation history to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67dfd299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_history(message, history):\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = ollama.chat.completions.create(model='gemma3', messages=messages)\n",
    "    return response.choices[0].message.content, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11b8ed07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Adam!\n",
      "It‚Äôs lovely to meet you, Adam from Taiwan ‚Äì I‚Äôm doing well and ready to assist you with anything you need!\n",
      "As a helpful AI assistant, I don‚Äôt have personal experiences or a location, but I‚Äôm ready to assist you ‚Äì and it‚Äôs nice to meet you, Adam! To tell you where you live, I need you to tell me! üòä\n"
     ]
    }
   ],
   "source": [
    "start_conversation = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?, my name is Adam.\"},\n",
    "    \n",
    "    ]\n",
    "\n",
    "reply1, history = chat_with_history(\"What is my name?\", start_conversation)\n",
    "print(reply1)\n",
    "\n",
    "reply2, history = chat_with_history(\"I live in Taiwan\", history)\n",
    "\n",
    "print(reply2)\n",
    "\n",
    "reply3, history = chat_with_history(\"Where do I live?\", history)\n",
    "print(reply3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
